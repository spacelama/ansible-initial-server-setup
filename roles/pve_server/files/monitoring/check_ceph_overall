#!/bin/bash

trap fail ERR

fail=false

failed_tests=

function fail() {
    code=$? ; CMD="$BASH_COMMAND"

    # FIXME: I should probably keep separate track over just warnings,
    # and still actually warn about them, but for now, we won't exit
    # failure code if we just recieve warnings
    case "$code" in
        1)
            echo " * -> Warning($code): $CMD"
            ;;
        *)
            echo " * -> Failed($code): $CMD"

            failed_tests="$failed_tests$CMD ; "
            fail=true
            ;;
    esac
    echo "*** Overall CEPH status not optimal.  Check the health status including 'dmesg' for individual hosts to work out device at fault"
}

cd /usr/local/bin/nagios

fss=$( ceph fs ls | sed 's/,//' | awk '{print $2}' )
pools=$( ceph osd pool ls )
hostname=$( hostname )

for pool in $pools ; do
    ./check_ceph_df -W 90 -C 95 -p $pool
    ! rados list-inconsistent-pg $pool | jq -r '.[]' | grep .
done
./check_ceph_health
# a and b, but they're all on a standby arrangement and some aren't
# up.  I'm not actually quite sure of what defines healthy, so let's
# keep trying!
for mds in `hostname`-a `hostname`-b ; do # each worker needs to be
                                          # responsible for something?
    success=false
    for fs in $fss ; do
        if mds_status=$(./check_ceph_mds -n $mds -f $fs) ; then
            success=true
        fi
        echo "${mds_status/MDS ERROR:/(suppressed) MDS ERROR:}"
    done
    if ! $success ; then
        echo $mds ERROR on all of: ./check_ceph_mds -n $mds -f all of: $fss
        fail=true
    fi
done
./check_ceph_mgr
./check_ceph_mon -I $hostname
./check_ceph_osd -H $hostname
./check_ceph_osd_df -W 90 -C 95
#./check_ceph_rgw  # rados gateway to S3

if $fail ; then
    echo Failed tests: $failed_tests
    exit 1
fi
