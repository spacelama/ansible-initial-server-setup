---
- name: Purge some incompatible packages
  apt:
    name: ['ifupdown']   # some stale config files left from automatic
                         # removal of ifupdown stop network from
                         # coming up, so make sure the package is
                         # purged first (ifupdown2 needs to be
                         # reinstalled afterwards, so let's just try
                         # to do it second anyway.  Could leave with
                         # an unbootable system if that fails!)
    state: absent
    purge: true
  become: true

- name: Purge ineffective packages
  apt:
    name: ['ksm-control-daemon'] # ksm is constantly eating a fair
                                 # amount of CPU.  We don't expect
                                 # many shared pages on our VMs
                                 # (especially on pve2), and
                                 # /sys/kernel/mm/ksm/general_profit
                                 # is large and negative on pve2
                                 # (where we're constantly swapping
                                 # too, and turning this off seems to
                                 # have *drastically* dropped
                                 # swapin/swapout rates from ~5 down
                                 # to 1MB/s) and 0 elsewhere:
                                 # https://docs.kernel.org/next/admin-guide/mm/ksm.html
                                 # https://forum.proxmox.com/threads/ksm-control-daemon-vs-ksmtuned-installation.81262/
    state: absent
    purge: true
  become: true

- name: Install pve prereq packages
  apt:
    name:
      - 'dkms'
      - 'ifupdown2'
      - >
         {{ ( deb_release | default('bullseye') == 'bullseye' ) |
         ternary ( 'pve-headers', 'proxmox-default-headers' )}}
    autoremove: no
    state: present
    install_recommends: no
  become: true

- name: determine if vendor-reset dkms module already exists
  command: /usr/sbin/dkms status -m vendor-reset -k {{ ansible_kernel }}
  register: vendor_reset_dkms
  changed_when: false

- debug:
    msg: "dkms status vendor-reset: {{ vendor_reset_dkms.stdout }}"
  when: vendor_reset_dkms.stdout is defined

  # disabled because we actually need the old kernels on pve1 for when we want to be able to lvcreate: https://forum.proxmox.com/threads/lvm-lvcreate-getting-stuck-in-pve8-kernel-on-some-jbod-controller-ssd-intel-dc-s3500-combinations.134649/
# - name: Purge old kernels in bookworm
#   apt:
#     name: ['pve-kernel-5.15*', 'pve-headers-5.15*' ]
#     state: absent
#     purge: true
#   become: true
#   when: deb_release | default('bullseye') == 'bookworm'

  # zfs root systems in pve8 might need systemd-boot package:
  # https://pve.proxmox.com/wiki/Roadmap#8.0-known-issues
- name: Install pve prereq and handy packages for pve8
  apt:
    name: ['systemd-boot', 'systemd-zram-generator' ]
    autoremove: no
    state: present
    install_recommends: no
  become: true
  when: deb_release | default('bullseye') == 'bookworm'

- name: Ensures ~ansible/Downloads/vendor-reset-084881c dir exists
  file:
    path: "/home/{{ ansible_env.USER }}/Downloads/vendor-reset-084881c"
    state: directory

- name: copy AMD graphics vendor-reset to pve
  unarchive:
    src: /home/tconnors/code/vendor-reset-084881c.tar
    dest: "/home/{{ ansible_env.USER }}/Downloads/vendor-reset-084881c"
    extra_opts:
    - --transform
    - s/^vendor-reset//
  register: vendor_reset_changed
  # want to rethink this, but in check_mode, we can't hope to unpack
  # the archive when the user doesn't yet exist
  when: not ansible_check_mode and not ( "installed" in vendor_reset_dkms.stdout )

- name: work around bug in dkms where it can't install something that didn't build last time, even with --force supplied
#  command: bash -c 'cd Downloads/vendor-reset-084881c ; dkms install .'
  command: dkms remove vendor-reset/0.1.1
  args:
    chdir: /home/{{ ansible_env.USER }}/Downloads/vendor-reset-084881c
  become: true
  # want to rethink this, but in check_mode, we can't hope to unpack
  # the archive when the archive isn't yet unpacked
  when: not ansible_check_mode and ( not ( "installed" in vendor_reset_dkms.stdout ) or vendor_reset_changed.changed )
  ignore_errors: true  # fails when not in systemd, or still in the
                       # installation environment, etc

- name: install AMD graphics vendor-reset to pve
#  command: bash -c 'cd Downloads/vendor-reset-084881c ; dkms install .'
  command: dkms install .
  args:
    chdir: /home/{{ ansible_env.USER }}/Downloads/vendor-reset-084881c
  become: true
  # want to rethink this, but in check_mode, we can't hope to unpack
  # the archive when the archive isn't yet unpacked
  when: not ansible_check_mode and ( not ( "installed" in vendor_reset_dkms.stdout ) or vendor_reset_changed.changed )
  ignore_errors: true  # fails when not in systemd, or still in the
                       # installation environment, etc

- name: Install virtual host tools
  apt:
    # https://pve.proxmox.com/wiki/Install_Proxmox_VE_on_Debian_Jessie
    name: ['proxmox-ve', 'ssh', 'postfix', 'open-iscsi', 'systemd-sysv', 'munin-libvirt-plugins', 'targetcli-fb', 'powertop', 'libguestfs-tools', 'hwloc', 'numad', 'emacs'] # 'virt-goodies', (emacs because big enough servers, and use it often enough...)
#, 'swapspace']
#    FIXME: run tuned-adm profile powersave or perhaps virtual-host # https://www.reddit.com/r/Proxmox/comments/uc53m7/4port_j4125_pc_how_to_run_proxmox_with_lowpower/
    update_cache: yes
    cache_valid_time: 3600
    autoremove: no
    state: present
  become: true

# - name: mount jellyfin
#   mount:
#     path: /jellyfin
#     src: /mnt/pve/cephfs-hdd/media/jellyfin
#     fstype: none
#     opts: bind,_netdev,x-systemd.automount
#     state: present
#   become: true
#   ignore_errors: true  # fails when not in systemd, or still in the
#                        # installation environment, etc
#   notify: systemd daemon reload

# - name: mount media transcodes
#   mount:
#     path: /media_transcodes
#     src: /mnt/pve/cephfs-hdd/media/transcodes
#     fstype: none
#     opts: bind,_netdev,x-systemd.automount
#     state: present
#   become: true
#   ignore_errors: true  # fails when not in systemd, or still in the
#                        # installation environment, etc
#   notify: systemd daemon reload

# - name: Ensures {{ item }} exists
#   file:
#     path: "{{ item }}"
#     state: directory
#   become: true
#   with_items:
#     - /etc/systemd/system/jellyfin.automount.d
#     - /etc/systemd/system/media_transcodes.automount.d

# - name: make sure jellyfin isn't mounted til ceph is mounted
#   copy:
#     dest: /etc/systemd/system/jellyfin.automount.d/override.conf
#     content: "[Unit]\nAfter=mnt-pve-cephfs\x2dhdd.mount\n"
#   become: true
#   notify: systemd daemon reload

# - name: make sure media_transcodes isn't mounted til ceph is mounted
#   copy:
#     dest: /etc/systemd/system/media_transcodes.automount.d/override.conf
#     content: "[Unit]\nAfter=mnt-pve-cephfs\x2dhdd.mount\n"
#   become: true
#   notify: systemd daemon reload

# Much better way of automounting a remote NFS directory before
# turning on the LXC seems to be to getting automounter to mount it
# somewhere appropriate, then present that through the LXC using
# resource mounts, with shared=1; eg, see lxc 129 webserver or 105
# media

- name: ensure SMR drives have a longer timeout
  copy:
     content: "ACTION==\"add\", SUBSYSTEM==\"block\",  ENV{ID_MODEL}==\"ST8000AS0002*\", RUN+=\"/bin/sh -c 'echo 60 > /sys/block/%k/device/timeout'\"\n"
     # find string with `udevadm test /sys/class/block/sdf`
     # also tells you whether it appended RUN appropriately
     dest: /etc/udev/rules.d/90-SMR-drive-timeout.rules
  become: true

- name: install suspend-all
  copy:
    src: pve_server/{{ item }}
    dest: /usr/local/bin/{{ item }}
    mode: 0755
  become: true
  with_items:
    - suspend-all

# FIXME: limit zfs RAM usage to 8GB on pve itself if the only timely stuff its going to host is on SSD anyway: https://pve.proxmox.com/wiki/ZFS_on_Linux#sysadmin_zfs_limit_memory_usage

# FIXME: We don't have any local storage on filesystems that aren't
# ZFS, so there's almost no point to setting up file based swap:
#- name: Ensures /var/lib/swapspace 0700
#  file:
#    path="/var/lib/swapspace"
#    state=directory
#    mode=0700
#  become: true

- name: install ceph etc munin monitoring plugins from munin-contrib, that don't take arguments
  copy:
    src: munin/{{ item }}
    dest: /etc/munin/plugins/
    owner: root
    group: root
    mode: 0755
    local_follow: true
  become: true
  with_items:
   - ceph_osd
   - ceph_capacity
#   - ceph-osd-info
   - proxmox_vm_count
  notify: restart munin-node

- name: install some extra ceph monitoring plugins
  copy:
    src: monitoring/{{ item }}
    dest: /usr/local/bin/nagios/
    owner: root
    group: root
    mode: 0755
    local_follow: true
  become: true
  with_items:
   - check_ceph_df
   - check_ceph_health
   - check_ceph_mds
   - check_ceph_mgr
   - check_ceph_mon
   - check_ceph_osd
   - check_ceph_osd_db
   - check_ceph_osd_df
   - check_ceph_overall

- name: install /usr/local/bin/ceph-deep-scrub-pg-ratio
  copy:
    src: usr/local/bin/ceph-deep-scrub-pg-ratio
    dest: /usr/local/bin/ceph-deep-scrub-pg-ratio
    mode: 0755
  become: true

- name: install ceph-deep-scrub-pg-ratio cronjob
  copy:
    src: etc/cron.d/ceph-scrub-deep
    dest: /etc/cron.d/ceph-scrub-deep
    mode: 0644
  become: true
  notify: restart cron

- name: set pve-ha-lrm/pve-ha-crm disabled when on HA or enabled when on HA, to alleviate constant disk writes by pmxcfs to SSD
  # https://forum.proxmox.com/threads/pmxcfs-writing-to-disk-all-the-time.35828/ https://www.reddit.com/r/Proxmox/comments/i8e5fi/excessive_nvme_wear_out/g193xr2/
  service:
    name: "{{ item }}"
    enabled: "{{ ( host_is_in_pve_cluster | default(false) and host_is_in_pve_cluster ) | ternary('yes', 'no') }}"
    state: "{{ ( host_is_in_pve_cluster | default(false) and host_is_in_pve_cluster ) | ternary('started', 'stopped') }}"
  become: true
  with_items:
    - pve-ha-lrm
    - pve-ha-crm
  ignore_errors: true  # fails when not in systemd, or still in the
                       # installation environment, etc

- name: reduce frequent writes by pvesr.timer
  # https://forum.proxmox.com/threads/replication-runner-syslog.35600/
  copy:
    content: "[Unit]\nDescription=Proxmox VE replication runner\n\n[Timer]\nAccuracySec=1\nRemainAfterElapse=no\n\n[Timer]\nOnCalendar=*:0/15\n\n[Install]\nWantedBy=timers.target\n"
    dest: /etc/systemd/system/pvesr.timer
  become: true

- name: Setup pve users
  copy:
     content: "user:root@pam:1:0:::tim.w.connors@gmail.com:::\n{{ pve_users | join('\n') }}\n"
     dest: /etc/pve/user.cfg
     mode: "0640"
     unsafe_writes: true # needed because of the /etc/pve fuse filesystem
  become: true

#FIXME: keep monitoring lifetime left, and if drops below 0 (or Percent_Lifetime_Remain raw value goes above 100 on its way to 255, which then makes lifetime writes somewhere around 264TB, which is much closer to TBW), then it's just a SMART bug and we can back out this timeout back to default 5.  Also, if doesn't drop its rate, might as well back it out too and find some other source of write amplification in the fs VM # https://www.reddit.com/r/zfs/comments/cott44/how_can_i_check_to_make_sure_i_dont_have_a_write/ewqjld0/
- name: "zfs module parameters"
  copy:
    dest: /etc/modprobe.d/zfs-custom.conf
    content: |
      # ANSIBLE CONTROLLED
      # by pve_server/main.yml
      # to ensure SSD doesn't have too many writes:
      options zfs zfs_txg_timeout=15
      # now that our VMs are entirely served out of ceph, zfs is only
      # for our zfs root, and we want to limit caching much lower than
      # zfs's default limits.  Without this we get 10MB/s constant
      # swapping in and out on pve2:
      # https://rather.puzzling.org/munin/pve2/pve2/diskstats_throughput/md127.html
      options zfs zfs_arc_min=53687091
      # 50MB
      options zfs zfs_arc_max=107374182
      # 100MB
  become: true
  notify: "Update initramfs config"

# if need to regenerate pve boot config because of new disks,
# procedure is here: https://pve.proxmox.com/wiki/Host_Bootloader

- name: install ssh socket directory
  file:
    path: "/root/.ssh/cm_master"
    state: directory
  become: true

- name: ensure ssh config exists
  #FIXME: The default in a fresh proxmox deploy is actually "Ciphers aes128-ctr,aes192-ctr,aes256-ctr,aes128-gcm@openssh.com,aes256-gcm@openssh.com,chacha20-poly1305@openssh.com"
  file:
    path: /root/.ssh/config
    state: touch
    modification_time: preserve
    access_time: preserve
  become: true

- name: install ssh config
  lineinfile:
    dest: /root/.ssh/config
    line: "{{ item }}"
  with_items:
    - "ControlMaster auto"
    - "ControlPath ~/.ssh/cm_master/%r@%h:%p"
    - "ControlPersist yes"
  become: true

- name: "Install guest snippets directory"
  file:
    path: "/var/lib/vz/snippets/"
    state: directory
  become: true

- name: "Install guest snippets"
  copy:
    src: guest-hookscript.pl
    dest: /var/lib/vz/snippets/guest-hookscript.pl
    mode: 0755
  become: true

  # FIXME: consider also reducing rrd writes: https://forum.proxmox.com/threads/reducing-rrdcached-writes.64473/
# (but primarily, suspect most writes are coming from rrd in fs)


# https://pve.proxmox.com/wiki/Pci_passthrough

  # test devices you want to pass through belong in separate IOMMUs:
#https://www.reddit.com/r/SolusProject/comments/955osy/vfio_passthrough_quick_reference/
##!/bin/bash
#shopt -s nullglob
#for d in /sys/kernel/iommu_groups/*/devices/*; do
#    n=${d#*/iommu_groups/*}; n=${n%%/*}
#    printf 'IOMMU Group %s ' "$n"
#    lspci -nns "${d##*/}"
#done;

# FIXME: run `proxmox-boot-tool status` after pve7to8 before reboot!

- name: "Append intel_iommu=on boot config"
  include_tasks: "{{ playbook_dir }}/roles/common/tasks/add_boot_flag.yml"
  loop_control:
    loop_var: flag
  when: host_is_intel | default(false)
  with_items: [ 'intel_iommu=on', 'iommu=pt', 'pcie_acs_override=downstream,multifunction' ]

- name: "Append amd_iommu=on boot config"
  include_tasks: "{{ playbook_dir }}/roles/common/tasks/add_boot_flag.yml"
  loop_control:
    loop_var: flag
  when: host_is_amd | default(false)
  with_items: [ 'amd_iommu=on', 'iommu=pt', 'pcie_acs_override=downstream,multifunction' ]

# - name: "host modules blacklist (radeon, snd_hda_intel, amdgpu, usb3)"
#   copy:
#     dest: /etc/modprobe.d/pve-custom-blacklist.conf
#     content: "# ansible controlled for RX550 by pve_server/main.yml\nblacklist radeon\nblacklist amdgpu\nblacklist snd_hda_intel\nblacklist xhci_hcd\nblacklist xhci_pci\n"
#   become: true
#   notify: "Update initramfs config"

- name: "Append modprobe blacklist boot config"
  include_tasks: "{{ playbook_dir }}/roles/common/tasks/add_boot_flag.yml"
  loop_control:
    loop_var: flag
  with_items:
    - key: "modprobe.blacklist"
      value: "{{ modprobe_blacklist }}"
      comment: "separate out all IOMMU groups, blacklist all the desktop functions we want to passthrough to VM, and override the blacklist from the rescue variable, GRUB_CMDLINE_LINUX"
  when: modprobe_blacklist is defined

- name: "Remove modprobe blacklist boot config"
  include_tasks: "{{ playbook_dir }}/roles/common/tasks/remove_boot_flag.yml"
  loop_control:
    loop_var: flag
  with_items: [ 'modprobe.blacklist' ]
  when: modprobe_blacklist is not defined

  # our default pve passes through USB ports via vfio, so you can't
  # use a keyboard in a rescue situation!  Make the boot rescue entry
  # not load vfio and not blacklist the USB modules.  Other rescue
  # boot flag possibilities are: "systemd.unit=rescue.target", "1",
  # "systemd.unit=emergency.target", or rescue via some other means
  # and add a debug shell to tty9 via `systemctl enable
  # debug-shell.service` or systemd.debug-shell=1 on the cmdline:
  # https://wiki.debian.org/systemd (other debug tips can be found
  # here: https://freedesktop.org/wiki/Software/systemd/Debugging/
  # Help on systemd targets found here:
  # https://opensource.com/article/20/5/systemd-startup )
- name: "Add modprobe blacklist to failsafe boot config"
  include_tasks: "{{ playbook_dir }}/roles/common/tasks/add_boot_flag.yml"
  loop_control:
    loop_var: flag
  vars:
    recovery_systemdboot: true # We're modifying the backup boot
                               # option, $GRUB_CMDLINE_LINUX.
                               # systemd-boot doesn't have this as
                               # equivalent, so we're going to have to
                               # modify our boot scripts (below)
  with_items:
    - key: "modprobe.blacklist"
      value: "vfio_pci"
      comment: "if vfio stops us from booting, allow rescue mode to boot up"

- name: "Install custom boot flag generator"
  copy:
    src: etc/kernel/postinst.d/zz-zz-install-recovery-options
    dest: /etc/kernel/postinst.d/zz-zz-install-recovery-options
    mode: 0755
  become: true
  notify: "Regenerate boot config"

- name: "vfio module parameters"
  copy:
    dest: /etc/modprobe.d/vfio-custom.conf
    # locations obtained with lspci -nvvv
    content: "# ansible controlled to passthrough amdgpu and usb card, by pve_server/main.yml\noptions vfio-pci {{ vfio_ids }}\n"
  become: true
  notify: "Update initramfs config"
  when: vfio_ids is defined

- name: "vfio module parameters"
  file:
    dest: /etc/modprobe.d/vfio-custom.conf
    state: absent
  become: true
  notify: "Update initramfs config"
  when: vfio_ids is not defined

- name: "Ensure vfio, vendor-reset in /etc/modules"
  lineinfile:
    dest: /etc/modules
    line: "{{ item }}"
    state: present
  become: true
  notify: "Update initramfs config"
  with_items:
    - "# ansible controlled for GPU passthrough and reset by pve_server/main.yml"
    - vfio
    - vfio_iommu_type1
    - vfio_pci
    - vfio_virqfd
    # following for amd video cards in VMs
    - vendor-reset
    - "# following modules are blacklisted on the kernel commandline to stop from autoloading, but now that vfio is loaded reserving the appropriate devices, we are free to load them again (nope, have to probe them manually)"
    - "#xhci_pci"
    - "#xhci_hcd"

# persistent network names (lan0): https://wiki.debian.org/NetworkInterfaceNames
- name: install systemd network persistent net link
  template:
    src: systemd-network-persistent-net-link.j2
    dest: /etc/systemd/network/10-persistent-net.link
  become: true

- name: install network interfaces
  template:
    src: network-interfaces.j2
    dest: /etc/network/interfaces
  become: true

