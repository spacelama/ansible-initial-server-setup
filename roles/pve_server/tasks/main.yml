---
- name: determine if enterprise pve subscription exists
  stat: path=/etc/apt/apt.conf.d/86pve-nags
  register: pve_enterprise_nag_buster

- name: determine if pve community repository present
  stat: path=/etc/apt/sources.list.d/pve-community.list
  register: pve_community_repository

- name: determine if pve enterprise repository present
  stat: path=/etc/apt/sources.list.d/pve-enterprise.list
  register: pve_enterprise_repository

- name: ensure old pve community repository removed
  command: mv /etc/apt/sources.list.d/pve-community.list /etc/apt/sources.list.d/pve-community.list.disabled
  become: true
  when: pve_community_repository.stat.exists

- name: ensure original pve enterprise repository removed
  command: mv /etc/apt/sources.list.d/pve-enterprise.list /etc/apt/sources.list.d/pve-enterprise.list.disabled
  become: true
  when: pve_enterprise_repository.stat.exists

- name: Ensures ~ansible/Downloads dir exists
  file:
    path="/home/{{ ansible_env.USER }}/Downloads"
    state=directory

- name: copy pve-nag-buster/install.sh to pve
  copy:
    src: /home/tconnors/code/pve-nag-buster/install.sh
    dest: "/home/{{ ansible_env.USER }}/Downloads/pve-nag-buster-install.sh"
    mode: 0755
  register: pve_nag_buster_changed

- name: install the pve nag buster
  command: /home/{{ ansible_env.USER }}/Downloads/pve-nag-buster-install.sh --offline
  when: not pve_enterprise_nag_buster.stat.exists or pve_nag_buster_changed.changed
  become: true

- name: remove prohibited packages
  apt:
    name: ['os-prober']
    state: absent
  become: true

- name: Install proxmox tools
  apt:
    # https://pve.proxmox.com/wiki/Install_Proxmox_VE_on_Debian_Jessie
    name: ['proxmox-ve', 'ssh', 'postfix', 'ksm-control-daemon', 'open-iscsi', 'systemd-sysv', 'virt-goodies', 'munin-libvirt-plugins']
#, 'swapspace']
    update_cache: yes
    cache_valid_time: 3600
    autoremove: no
    state: present
  become: true

- name: ensure SMR drives have a longer timeout
  copy:
     content: "ACTION==\"add\", SUBSYSTEM==\"block\",  ENV{ID_MODEL}==\"ST8000AS0002*\", RUN+=\"/bin/sh -c 'echo 60 > /sys/block/%k/device/timeout'\"\n"
     # find string with `udevadm test /sys/class/block/sdf`
     # also tells you whether it appended RUN appropriately
     dest: /etc/udev/rules.d/90-SMR-drive-timeout.rules
  become: true

- name: install suspend-all
  copy:
    src: ./src/pve_server/{{ item }}
    dest: /usr/local/bin/{{ item }}
    owner: root
    group: root
    mode: 0755
  become: true
  with_items:
    - suspend-all

  # FIXME: there are no partitions on pve where there's enough space for swapspace to operate
#- name: Ensures /var/lib/swapspace 0700
#  file:
#    path="/var/lib/swapspace"
#    state=directory
#    mode=0700
#  become: true

- name: ensure all munin smart links are in place
  file:
    src: /usr/share/munin/plugins/smart_
    dest: "/etc/munin/plugins/smart_sd{{ item }}"
    state: link
  become: true
  with_items: { a, b, c, d, e, f, g, h, i, j, k, l, m, n }

- name: disable pve-ha-lrm/pve-ha-crm on a non HA setup, to alleviate constant disk writes by pmxcfs to SSD
  # https://forum.proxmox.com/threads/pmxcfs-writing-to-disk-all-the-time.35828/ https://www.reddit.com/r/Proxmox/comments/i8e5fi/excessive_nvme_wear_out/g193xr2/
  service:
    name: "{{ item }}"
    enabled: no
    state: stopped
  become: true
  with_items:
    - pve-ha-lrm
    - pve-ha-crm

- name: reduce frequent writes by pvesr.timer
  # https://forum.proxmox.com/threads/replication-runner-syslog.35600/
  copy:
    content: "[Unit]\nDescription=Proxmox VE replication runner\n\n[Timer]\nAccuracySec=1\nRemainAfterElapse=no\n\n[Timer]\nOnCalendar=hourly\n\n[Install]\nWantedBy=timers.target\n"
    dest: /etc/systemd/system/pvesr.timer
  become: true

  # FIXME: consider also reducing rrd writes: https://forum.proxmox.com/threads/reducing-rrdcached-writes.64473/
# (but primarily, suspect most writes are coming from rrd in fs)

# https://pve.proxmox.com/wiki/Pci_passthrough
- name: "Search for iommu grub config"
  become: yes
  register: iommu_grub_cmdline_exists
  check_mode: yes # cause this to become just a test.  If there's already
                  # iommu settings, then this will think line is
                  # being replaced, and changed will become true (but we
                  # force it to false to not output a line saying
                  # "changed"), and msg will become "line added", else
                  # changed stays false, and msg does not contain "line
                  # added"
  lineinfile:
    dest: /etc/default/grub
    line: grub cmdline already has iommu mitigations disabled
    regexp: "^GRUB_CMDLINE_LINUX_DEFAULT=.*intel_iommu=on"
    state: present
  changed_when: false


  # test devices you want to pass through belong in separate IOMMUs:
#https://www.reddit.com/r/SolusProject/comments/955osy/vfio_passthrough_quick_reference/
##!/bin/bash
#shopt -s nullglob
#for d in /sys/kernel/iommu_groups/*/devices/*; do
#    n=${d#*/iommu_groups/*}; n=${n%%/*}
#    printf 'IOMMU Group %s ' "$n"
#    lspci -nns "${d##*/}"
#done;
- name: "Append intel_iommu=on grub config"
  when: iommu_grub_cmdline_exists.msg == "line added"
  lineinfile:
    dest: /etc/default/grub
    backrefs: yes
    regexp: "^GRUB_CMDLINE_LINUX_DEFAULT=\"(.*)\""
    line: "GRUB_CMDLINE_LINUX_DEFAULT=\"\\1 intel_iommu=on iommu=pt\""
    state: present
  become: true
  notify: "Regenerate grub config"

# if need to regenerate grub boot config because of new disks, procedure is here: https://pve.proxmox.com/wiki/Host_Bootloader

- name: "host modules blacklist (radeon, snd_hda_intel, amdgpu, usb3)"
  copy:
    dest: /etc/modprobe.d/pve-custom-blacklist.conf
    content: "# ansible controlled for RX550 by pve_server/main.yml\nblacklist radeon\nblacklist amdgpu\nblacklist snd_hda_intel\nblacklist xhci_hcd\nblacklist xhci_pci\n"
  become: true
  notify: "Update initramfs config"

#FIXME: keep monitoring lifetime left, and if drops below 0 (or Percent_Lifetime_Remain raw value goes above 100 on its way to 255, which then makes lifetime writes somewhere around 264TB, which is much closer to TBW), then it's just a SMART bug and we can back out this timeout back to default 5.  Also, if doesn't drop its rate, might as well back it out too and find some other source of write amplification in the fs VM # https://www.reddit.com/r/zfs/comments/cott44/how_can_i_check_to_make_sure_i_dont_have_a_write/ewqjld0/
- name: "zfs module parameters"
  copy:
    dest: /etc/modprobe.d/zfs-custom.conf
    content: "# ansible controlled to ensure SSD doesn't have too many writes, by pve_server/main.yml\noptions zfs zfs_txg_timeout=15\n"
  become: true
  notify: "Update initramfs config"

- name: "vfio module parameters"
  copy:
    dest: /etc/modprobe.d/vfio-custom.conf
    content: "# ansible controlled to passthrough amdgpu and usb card, by pve_server/main.yml\noptions vfio-pci ids=1002:699f,1002:aae0,1106:3483\n"
  become: true
  notify: "Update initramfs config"

- name: "Ensure vfio, vendor-reset in /etc/modules"
  lineinfile:
    dest: /etc/modules
    line: "{{ item }}"
    state: present
  become: true
  notify: "Update initramfs config"
  with_items:
    - "# ansible controlled for GPU passthrough and reset by pve_server/main.yml"
    - vfio
    - vfio_iommu_type1
    - vfio_pci
    - vfio_virqfd
    - vendor-reset

#FIXME: still need to upload vendor-reset in here

- name: "Install root crontab"
  copy:
    dest: /var/spool/cron/crontabs/root
    content: "@reboot echo powersave | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor\n"
    mode: 0600
  become: true
  notify: "reload cron"
