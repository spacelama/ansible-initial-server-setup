#!/bin/bash

# for invocation from /etc/cron.daily - grabs a snapshot of all sorts
# of dynamic system configuration that you might decide are important
# to restore in some sort of disaster recovery event.  swap, raid,
# lvm, zfs, network config, docker images, etc.

# General idea is there should be no output on stdout/stderr from any
# of the commands, and no command should exit with non-zero exit code,
# in normal operation.  This is not a monitoring script, some of the
# commands take some time to run so you probably only want to be
# running it once a day rather than continuously, and you might deploy
# it on hundreds of boxes (but you also want to know when we've
# configured a command with the wrong commandline options, so don't
# redirect stderr to the state file unless the command routinely
# outputs to stderr, and don't ignore errors with ` || true` unless
# the command exits non-0 routinely.

exec < /dev/null

function backup() {
# on hosts where files are synced back to a master server and version
# controlled every half hour, we can just symlink 'rsync -L'd files
# into recovery_data, and version control them.  Otherwise we might
# have to copy the file contents into this area with cp
    err_if_not_exist=1
    if [ "$1" = --noerr ] ; then
        shift
        err_if_not_exist=
    fi
    if [ -n "$err_if_not_exist" -o -e "$1" ] ; then
#        if [[ "$1" == /sys/* ]] || [[ "$1" == /proc/* ]] ; then
            dir="$(dirname "${@: -1}")"
            mkdir -p "$dir"
            cp -aL "$@"
#        else
#            ln -s "$@"
#        fi
    fi
}

function depth_dell_info() {
    local in_dell_field="$1"
    local dell_field

    header_len=`echo "$in_dell_field" | wc -c`
    printf "%$((header_len-1))s\\n" |tr " " "-"
    echo "$in_dell_field"
    printf "%$((header_len-1))s\\n" |tr " " "-"

    local dell_fields=$( /opt/dell/srvadmin/sbin/racadm get "$in_dell_field" )
    echo "$dell_fields" | while read dell_field ; do
        if [[ "$dell_field" == "$in_dell_field".* ]] ; then
            # sometimes you get the following, and only want
            # the first argument:
            # iDRAC.Users.1 [Key=iDRAC.Embedded.1#Users.1]

            dell_field=${dell_field%% *}

            # the field ends up printing itself plus some suffixes
            depth_dell_info "$dell_field"
        elif [ -n "$dell_field" ] && [[ "$dell_field" != *[\ =]* ]] ; then
            # no spaces, but not a subset of the current input, means
            # that you have to prepend the name of the input to these
            # fields
            depth_dell_info "$in_dell_field.$dell_field"
        else
            echo "$dell_field"
        fi
    done
    echo
}
function get_dell_info() {
    # this contains useful racadm commands:
    # https://github.com/tin6150/psg/blob/master/script/hpc/racadmSetBios.sh
#    local dell_fields=$( /opt/dell/srvadmin/sbin/racadm get ; echo BIOS )
    local dell_fields=$( echo System ; echo LifeCycleController ; echo iDRAC ; echo BIOS )
    local dell_field
    echo "$dell_fields" | while read dell_field ; do
        depth_dell_info "$dell_field"
    done > $data_dir/dell_racadm

    for omreport in 'chassis biossetup' 'chassis fans' 'chassis memory' 'chassis pwrsupplies' 'chassis pwrmanagement' 'chassis processors' 'chassis temps' 'chassis volts' 'chassis batteries' 'system summary' 'system version' 'storage pdisk controller=0' 'storage controller' 'storage battery' 'storage connector controller=0' ; do
        out="${omreport// /_}"
        /opt/dell/srvadmin/bin/omreport $omreport > $data_dir/dell_$out
    done
}

LANG=POSIX ; export LANG
PATH=/sbin:/bin:/usr/sbin:/usr/bin ; export PATH

exit=0

trap 'echo "ERROR: $hostname:$0: $LINENO: $BASH_COMMAND"; exit=1' ERR
hostname=`hostname`
date=`date`

data_dir_dir=/var/log
data_dir_base=conf_dump
data_dir_dest="$data_dir_dir/$data_dir_base"
data_dir="$data_dir_dest.tmp"
rm -rf "$data_dir"   #.tmp
mkdir "$data_dir"    #.tmp
chown root:tconnors "$data_dir" # things like monitor_iot.worker write into it as myself
chmod g+w,+t "$data_dir"        # but I can only delete my own things

echo "Host: $hostname" > $data_dir/info
echo "Date: $date" >> $data_dir/info

# Disk info

cat /proc/partitions > $data_dir/partitions
parted -l > $data_dir/parted 2>&1 || true

[ -e /dev/disk/by-id/ ] && ls -lA /dev/disk/by-id/ > $data_dir/dev.disk.by-id
fdisk -l > $data_dir/fdisk 2>&1

cat /etc/mtab > $data_dir/mtab
df -kT > $data_dir/df 2>&1 || true
df -Ti > $data_dir/df-i 2>&1 || true

for lvm in {vg,lv,pv}{display,scan} ; do
    $lvm -v > $data_dir/$lvm 2>&1 || true
done
# but override lvdisplay above:
lvdisplay -am > $data_dir/lvdisplay 2>&1 || true

# -D -f -t -M mix and matched:
lsblk -o \
      KNAME,MOUNTPOINTS,NAME,GROUP,TYPE,MAJ:MIN,LOG-SEC,MIN-IO,PHY-SEC,OPT-IO,FSAVAIL,FSUSE%,SIZE,LABEL,FSTYPE,FSVER,ROTA,SCHED,SUBSYSTEMS,HCTL,MODEL,PARTTYPENAME,UUID,HOTPLUG,ALIGNMENT,DISC-ALN,DISC-GRAN,DISC-MAX,DISC-ZERO,RQ-SIZE,RA,WSAME \
      -M    > $data_dir/lsblk   2>&1 || lsblk -D -f -t > $data_dir/lsblk   2>&1 || true
lsblk -O -M > $data_dir/lsblk-O 2>&1 || lsblk -O       > $data_dir/lsblk-O 2>&1 || true

backup --noerr /proc/mdstat $data_dir/mdstat

lvs -o+seg_all > $data_dir/lvs 2>&1 || true
pvs --segments -o+pvseg_all > $data_dir/pvs 2>&1 || true
lvs --segments -o+vg_all > $data_dir/lvs.vgall 2>&1 || true
vgs -o+vg_all > $data_dir/vgs 2>&1 || true
lvs -a -o +devices > $data_dir/lvs.devices 2>&1 || true
lvs --segments -a -o +lv_size,devices > $data_dir/lvs.sizes 2>&1 || true
pvs -o pv_name,pv_size,seg_size,vg_name,lv_name,lv_size,seg_pe_ranges > $data_dir/pvs.sizes 2>&1 || true

/usr/bin/lsscsi > $data_dir/lsscsi 2>&1 || true

if grep -q ^zfs /proc/modules ; then
    # FIXME: we're going to spin up disks doing this; it'd be nice to time it for when they're already spinning
    zpool status -v > $data_dir/zpool.status 2>&1 || true
    zfs get all > $data_dir/zfs.get.all 2>&1 || true
    zfs get -s local all > $data_dir/zfs.get.local.all 2>&1 || true
    zpool get all > $data_dir/zpool.get.all 2>&1 || true
    zpool list > $data_dir/zpool.list 2>&1 || true
    zpool list -v > $data_dir/zpool.list-v 2>&1 || true
    zfs list > $data_dir/zfs.list 2>&1 || true
    /usr/local/bin/zpool.sh > $data_dir/zfs.zpool.nice 2>&1 || true
    /usr/local/bin/zpool.sh iostat > $data_dir/zfs.zpool.iostat.nice 2>&1 || true
    /usr/local/bin/zfs_print_all > $data_dir/zfs-create.script 2>&1 || true
fi

for i in /dev/sd? /dev/nvme?n? ; do
    if [ -e "$i" ] ; then
        dev="${i#/dev/}"
        /usr/local/bin/smart-intercept-spindown -a "$i" > $data_dir/smartctl-a.$dev 2>&1 || true
        #FIXME: does this work for nvme and sas drives?
    fi
done

if [ -x /usr/sbin/nvme ] ; then
    nvme list "$i" > $data_dir/nvme.list
    for i in /dev/nvme?n? ; do
        if [ -e "$i" ] ; then
            dev="${i#/dev/}"
            nvme smart-log "$i" > $data_dir/nvme.smart-log.$dev
            nvme id-ctrl -H "$i" > $data_dir/nvme.id-ctrl.$dev
            # endurance-log? effects-log, self-test-log?
        fi
    done
fi

backup /proc/mounts $data_dir/mounts
backup /proc/swaps $data_dir/swaps
if [ -e /var/lib/nfs/etab ] ; then
    cat /var/lib/nfs/etab > $data_dir/nfsd.etab
fi
if [ -e /etc/multipath/bindings -a -e /sbin/multipath ] ; then
    /sbin/multipath -ll > $data_dir/multipath_ll || true
fi

# Network
if [ -x /usr/sbin/iptables ] ; then
    iptables -L -n > $data_dir/iptables
    iptables-save | grep -v -E '(Completed on|Generated by|ACCEPT.*\[.*:.*\])' > $data_dir/iptables-save || true
fi
if [ -x /usr/sbin/nft ] ; then
    nft list ruleset  > $data_dir/nftables.list-ruleset 2>&1
fi
if [ -x /sbin/ifconfig ] ; then
    ifconfig > $data_dir/ifconfig 2>&1
fi
if [ -x /bin/netstat ] ; then
    netstat -rn > $data_dir/netstat
fi
ip addr show > $data_dir/ipaddr 2>&1
(
    echo main
    ip route list table main
    echo local
    ip route list table local
) > $data_dir/route

mailq > $data_dir/mailq 2>&1

# current state of ethernet bonding:
if [ -e /sys/class/net/bonding_masters ] ; then
    for bond in `cat  /sys/class/net/bonding_masters` ; do
        ( cd /sys/class/net/$bond/bonding ; grep . * ; cd /proc/net/bonding/ ; grep . * ) > $data_dir/bond.$bond.status
    done
fi
# LLDP/CDP capture, ethernet status
for iface in `ip link | egrep "state UP" | awk '{print $2}'| egrep -v -e lo -e bond -e ib | sed 's/://g'` ; do
    #label each ethernet interface with the name of the bonded
    #interface it's part of if it's part of a bonded interface
    inbond=
    if [ -e /sys/class/net/bonding_masters ] ; then
        match="$(grep -H "$iface" /sys/class/net/*/bonding/slaves 2>/dev/null || true)"
        if [ -n "$match" ] ; then
            inbond="$(echo "$match" | sed 's!/sys/class/net/\(.*\)/bonding/slaves.*!.\1!')"
        fi
    fi
    ethtool "$iface" > $data_dir/ethtool."$iface" 2>&1 || true
    ethtool -k "$iface" > $data_dir/ethtool-k."$iface" 2>&1 || true

    # FIXME: perhaps we can make use of lldpd/lldpad
    # do all LLDP/CDP scans in paralell on this host since they take
    # up to 2 minutes.  Wait for them at the end
    #        timeout 120 tcpdump -v -s 1500 -c 1 -i "$iface" '(ether[12:2]=0x88cc or ether[20:2]=0x2000)' 2>&1 | grep -E "(System Name|Subtype Interface Name|0x0000:  0080)" > $data_dir/cdp"$inbond"."$iface" &
    timeout 120 tcpdump -v -s 1500 -c 1 -i "$iface" 'ether proto 0x88cc' 2>&1 | grep -E "(System Name|Subtype Interface Name|version|port vlan|aggregation status|MTU|0x0000:  0080)" > $data_dir/lldp"$inbond"."$iface" &
    timeout 120 tcpdump -v -s 1500 -c 1 -i "$iface" 'ether[20:2] == 0x2000' > $data_dir/cdp"$inbond"."$iface" 2>&1 &
done ; wait

# dpkg packages
dpkg --get-selections | sort > $data_dir/dpkg-packages 2>&1
if [ -x /usr/bin/apt-show-versions ] ; then
    apt-show-versions > $data_dir/apt-show-versions 2>&1
fi

# Dell
if [ -x /opt/dell/srvadmin/bin/omreport ] ; then
    get_dell_info
fi
if [ -x /usr/local/bin/megaclisas-status ] ; then
    /usr/local/bin/megaclisas-status > $data_dir/megaclisas-status
fi
# FIXME: other raid controller status?
# MegaCli64 AdpGetProp ... https://www.alteeve.com/w/MegaCli64_Cheat_Sheet

# Installation
#backup /root/anaconda-ks.cfg $data_dir/anaconda-ks.cfg

# Processes
#ls -la /usr/local > $data_dir/usr_local_list
#chkconfig --list > $data_dir/chkconfig_list 2>&1

if [ -x /bin/systemctl ] ; then
    systemctl -a > $data_dir/systemctl
    for i in 1 2 3 ; do
        systemctl list-unit-files > $data_dir/chkconfig_list.systemctl 2>&1 &&
            break   #likes to timeout on pi.  Works most of the time
                    #when invoked from cron outside of this script.
                    #Seems there's a hard-coded unreasonable timeout
                    #in systemd:
                    #https://github.com/systemd/systemd/issues/4985
    done
    systemctl --failed > $data_dir/systemctl.--failed
fi

ps axfu > $data_dir/ps.axfu
# grab the *second* sample for CPU usage
top -b -n 2 | tail -n +2 | sed -n '/^top/,$p' > $data_dir/top.usage

if [ -x /usr/bin/docker ] ; then
    docker image ls > $data_dir/docker.image.ls
    docker image ls -a > $data_dir/docker.image.ls-a
    docker container ls > $data_dir/docker.container.ls
    docker container ls -a > $data_dir/docker.container.ls-a
    docker ps > $data_dir/docker.ps
    docker ps -a > $data_dir/docker.ps-a
    # docker image prune
    # docker container prune
fi

if [ -x /usr/bin/snap ] ; then
    snap list > $data_dir/snap.list 2>&1
fi

# Kernel and boot

uname -a > $data_dir/uname 2>&1
#backup /boot/config-`uname -r` $data_dir/kernel-config
backup /proc/modules $data_dir/modules
backup /proc/cpuinfo $data_dir/cpuinfo
backup /proc/meminfo $data_dir/meminfo
lsmod > $data_dir/lsmod
lscpu > $data_dir/lscpu
lspci > $data_dir/lspci 2>/dev/null || true
lspci -vvv > $data_dir/lspci-vvv 2>/dev/null || true
if [ -x /usr/bin/cpupower ] ; then
    cpupower frequency-info > $data_dir/cpupower-frequency-info
fi
for d in /sys/kernel/iommu_groups/*/devices/*; do
    n=${d#*/iommu_groups/*}; n=${n%%/*}
    printf 'IOMMU Group %s ' "$n"
    lspci -nns "${d##*/}" || true
done > $data_dir/lspci_mmu 2>&1
if [ -x /usr/sbin/dmidecode ] ; then
    dmidecode > $data_dir/dmidecode 2>&1 || true
fi
if [ -x /usr/bin/fwupdmgr ] ; then
    fwupdmgr get-devices > $data_dir/fwupdmgr.get-devices 2>&1
fi
if [ -x /usr/bin/lsusb ] ; then
    lsusb > $data_dir/lsusb || true # can fail if no USB devices?
    lsusb -t > $data_dir/lsusb-t || true
fi
#backup /boot/grub/menu.lst $data_dir/grub
if ! [ -e /tmp/dmesg.boot ] ; then
    dmesg > /tmp/dmesg.boot 2>&1
fi
cp -p /tmp/dmesg.boot $data_dir/dmesg.boot
dmesg > $data_dir/dmesg 2>&1
backup --noerr /proc/pci $data_dir/pci
sysctl -a > $data_dir/sysctl 2> $data_dir/sysctl.err
backup /proc/cmdline $data_dir/cmdline

journalctl -b -p err | grep -v 'Journal begins' | ccze -m ansi > $data_dir/journal-errors

# Vendor
# copy the /etc/pve fuse filesystem that's built by proxmox based on
# its database, so we can recover from barebones if necessary
if [ -d /etc/pve ] ; then
    cp -pra /etc/pve $data_dir/pve

    if grep -q ceph /proc/mounts ; then
        ceph -s > $data_dir/ceph-s
        grep ^ceph: $data_dir/apt-show-versions > $data_dir/ceph-version
        /usr/local/bin/nagios/check_ceph_overall > $data_dir/ceph.nagios.health.overall

        # https://access.redhat.com/solutions/7010265
        # grab the *second* sample for CPU usage
        top -b -n 2 -p $( echo $( pgrep -f /usr/bin/ceph ) | sed 's/ /,/g' ) | tail -n +2 | sed -n '/^top/,$p' > $data_dir/ceph.top.daemon.usage
        ps -o rss,%mem,command ax | awk '{ if( $3 == "/usr/bin/ceph-osd") { cnt++; prc+=$2; byt+=$1; x=$1/1024/1024; $1=""; print x $0 }} END {print "Summary: " cnt " OSDs, Total Mem: " byt/1024/1024 "G, Percent: " prc;}' > $data_dir/ceph.osd.mem-usage
        for osd in $(ceph osd ls) ; do echo -n "osd.${osd}: " ; ceph config show-with-defaults osd.${osd} -f json | jq -c '.[] | (if .name == "osd_memory_target" or .name == "bluestore_cache_autotune" then .name +": " + .value + " " + .source  else empty end)' | sed -e 's/"//g' |  paste -d"\ " - - ; done  | column -t > $data_dir/ceph.osd.memory-settings 2>&1

        ceph balancer status > $data_dir/ceph.balancer.status
        ceph config dump     > $data_dir/ceph.config.dump
        # google: cephfs only 30MB/s read hdd 10gbit nic number of pgs
        ceph pg dump         > $data_dir/ceph.pg.dump 2>&1

        ceph-volume lvm list > $data_dir/ceph-volume.lvm.list
        osds_on_this_host=$( grep "osd id" $data_dir/ceph-volume.lvm.list | awk '{print $3}' | sort -g | uniq )

        ceph osd lspools | awk '{print $2}' | while read pool ; do
            rbd du -p $pool > $data_dir/ceph.du.rbd.pool.$pool
        done
        rados df > $data_dir/ceph.df.rados
        ceph osd pool ls detail > $data_dir/ceph.ls.osd.pool # grep full
        ceph osd pool autoscale-status > $data_dir/ceph.pool.autoscale-status
        ceph osd df > $data_dir/ceph.df.osd
        ceph osd df tree > $data_dir/ceph.df.osd.tree
        ceph df detail > $data_dir/ceph.df.detail

        ceph osd lspools | while read num pool ; do
            rbd ls $pool | while read volume ; do
                echo $pool/$volume:
                rbd snap ls $pool/$volume
                echo
            done
        done > $data_dir/ceph.pool.volume.snapshots

        # all OSDs across the cluster - for those commands you can reach from any node
        for osd in $( ceph osd df | grep '^ *[0-9]' | awk '{print $1}' ) ; do
            (
                for param in bdev_enable_discard bdev_async_discard_threads bluestore_use_optimal_io_size_for_min_alloc_size bluestore_min_alloc_size bluestore_min_alloc_size_hdd bluestore_min_alloc_size_ssd bluestore_max_alloc_size bluestore_compression_mode bluestore_compression_algorithm bluestore_compression_min_blob_size bluestore_compression_min_blob_size_hdd bluestore_compression_min_blob_size_ssd bluestore_compression_max_blob_size bluestore_compression_max_blob_size_hdd bluestore_compression_max_blob_size_ssd bluestore_max_blob_size bluestore_max_blob_size_hdd bluestore_max_blob_size_ssd ; do
                    printf "%60s" "$osd-$param = "
                    ceph config get osd.$osd $param
                done
            ) > $data_dir/ceph.osd.$osd.params
            ceph pg ls-by-osd osd.$osd > $data_dir/ceph.pg.ls.$osd
            ceph daemon osd.$osd perf dump > $data_dir/ceph.osd.$osd.perf.dump 2>/dev/null || true
            # whole bunch of interesting looking commands visible under:
            # ceph daemon osd.0 help
            # eg:
            #  - ceph daemon osd.6 bluefs stats
        done
        ceph osd lspools | awk '{print $2}' | while read pool ; do
            ceph osd pool get $pool all > $data_dir/ceph.pool.$pool.conf
        done

        # let's also look at some PVE ceph tools:
        pveceph pool ls > $data_dir/pveceph.pool.ls
        for osd in $osds_on_this_host ; do
            pveceph osd details $osd > $data_dir/pveceph.osd.details.$osd 2>&1 || true
            # verbose currently same as --output-format json-pretty
            pveceph osd details $osd --output-format json-pretty > $data_dir/pveceph.osd.details.$osd.verbose.json 2>&1 || true

            # and maybe also commands you only want to run on an OSD only a single time per day:
            # ceph tell osd.$osd bench > $data_dir/ceph.osd.bench.$osd  # but even then, this writes a GB per invocation.  Do we want to cause that level of excess wear per day if we're not actively tracking the results?
            # but tracking the IOPS would certainly be useful:
            ceph tell osd.$osd bench 4096 > $data_dir/ceph.osd.bench.$osd.4096
            # or `ceph tell osd.\* bench` to do it on all osds at the same time
        done
        for pool in $( pveceph pool ls --output-format json | jq -r .[].pool_name ) ; do
            pveceph pool get $pool > $data_dir/pveceph.pool.get.$pool 2>&1
            pveceph pool get $pool --output-format json-pretty > $data_dir/pveceph.pool.get.$pool.json 2>&1
        done
    fi
fi

if [ -x /usr/local/bin/get_conf_info.local ] ; then
    /usr/local/bin/get_conf_info.local "$data_dir" "$data_dir_dest"
fi

# Now, move all of the log files almost atomically to their final destination
if [ -d "$data_dir_dest" ] ; then
    rm -rf "$data_dir_dir/yesterday.$data_dir_base"
    mv "$data_dir_dest" "$data_dir_dir/yesterday.$data_dir_base"  # yesterday.conf_dump and not conf_dump.yesterday, because I'm tab completing to /var/log/conf_dump all the time
fi
mv "$data_dir" "$data_dir_dest"

exit $exit
